---
title: "Test R Markdown"
output: html_document
---

## Pull the Data

```{r DELETE_CHUNK}
library(pacman)
p_load('pins','tidyverse','skimr','tsibble','doParallel', 'DT', 'caret','anytime','gganimate','xgboost','tictoc')
```


The first thing we will need to do is download the latest data. We will do this by using the `pins` package [@R-pins], which has already been loaded into our session [in the previous section](#installing-and-loading-packages).

First, we will need to connect to a public GitHub repository (anyone can post their code to the GitHub website and make a "repository" with code for their project) and ***register*** the ***board*** that the data is ***pinned*** to by using the **`board_register()`** function:

```{r hitBTC-register}
board_register(name = "pins_board", url = "https://raw.githubusercontent.com/predictcrypto/pins/master/", board = "datatxt")
```

By running the `board_register()` command on the URL where the data is located, we can now ***"ask"*** for the data we are interested in, which is called `hitBTC_orderbook`:

```{r pull_data}
cryptodata <- pin_get(name = "hitBTC_orderbook")
```

The data has been saved to the `cryptodata` object.


## Data Preview {#data-preview}

```{r disable_scientific_notation, include=F}
options(scipen=999)
```

Below is a preview of the data:

```{r show_cryptodata, echo=F, message=FALSE, warning=FALSE}
library(DT)
datatable(head(cryptodata, 2000),  style = "default", 
          options = list(scrollX = TRUE, pageLength=3, lengthMenu = c(3, 5, 10)), rownames = F)
```

This is [***tidy data***](https://tidyr.tidyverse.org/articles/tidy-data.html), meaning:

1.  Every column is a variable.

2.  Every row is an observation.

3.  Every cell is a single value.

The data is collected once per hour and includes 120 days worth of data relative to today's date. Each row is an observation of an individual cryptocurrency, and the same cryptocurrency is tracked on an hourly basis, each time presented as a new row in the dataset.

<br>

Only the first 2,000 rows of the data are shown in the table above. There are `r nrow(cryptodata)` rows in the actual full dataset. The latest data is from `r max(cryptodata$date, na.rm =T )`.



## The definition of a "price"

[ADD HERE ABOUT ORDERBOOKS VS. PRICE SHOWN ON A WEBSITE]

The price of a cryptocurrency on an exchange is given by the [***order book***](https://www.investopedia.com/terms/o/order-book.asp), where traders can post trades they want to perform. [KEEP ADDING HERE]

<!-- A trade can be posted as a ***market*** order to purchase or sell the cryptocurrency at the currently most favori -->








## Data Quality {#data-quality}

First, let us get a general overview of the data to confirm there are no major data quality issues. There are several ways to do this, including using base R functions (base R meaning anything that comes with the default installation of R when you start up a new session), but we will use the `skimr`[@R-skimr] package to get a nicely formatted output.

We can use `skim()` on the `cryptodata` dataframe to get a summary of the data to help locate any potential problems:

```{r skimr}
skim(cryptodata)
```

<!-- ```{r show_data_preview, echo=F} -->

<!-- # try this post-call -->

<!-- knitr::kable(data_quality_overview, longtable = TRUE, booktabs = TRUE) -->

<!-- ``` -->

<!-- The new object we just created and displayed `data_quality_overview` contains a lot of useful information pertaining to the quality of the data. The column `complete_rate` for example, gives the percentage of non-missing values for each of the columns in the `cryptodata` data. While the output is slightly cut off on the right side of the document, you can see the full output by running the code on your computer. What is even neater about the object we made through `skimr`, is that the result itself is a dataframe that we can use to make adjustments to the data as needed. -->

<!-- We can see the result `data_quality_overview` that we displayed has columns of its own: -->

<!-- ```{r show_skimr_colnames} -->
<!-- colnames(data_quality_overview) -->
<!-- ``` -->

<!-- We can then use the information contained within this result to manipulate the data itself. For example we could exclude columns where 30% or more of the data is missing by applying the filter `complete_rate > 0.7` by using `subset()`: -->

<!-- ```{r remove_nulls_70_30_skimr} -->
<!-- subset(data_quality_overview, complete_rate > 0.7) -->
<!-- ``` -->



<!-- Think about how the resulting object would have less values for the column `skim_variable`, and how those values (meaning all columns with a complete rate greater than 70%) could then themselves be used to select which columns to remove out of the original dataset. We will use this idea in the [next section](#data-prep) to make the necessary adjustments to the data to dynamically get rid of any columns in the data that are missing too many values or don't have any variation in values. -->

<!-- Looking at this summary, you may have noticed the `Date` field is a character data type, so that's something else that we will need to convert in the [next section where we focus on the data preparation steps](#data-prep) before visualizing and modeling the data. -->

Move on to the [next section](#data-prep), where we make the adjustments necessary to the data before we can start making visualizations and predictive models.





## TEMP-DEL:




## Remove Nulls

[ADD HERE]

```{r omit_nulls}
cryptodata <- na.omit(cryptodata)
```


## Calculate `price_usd` Column

[ADD HERE]

Calculate the `price_usd` using the order books data and taking the cheapest price available from the **ask** side where at least $15 worth of the cryptocurrency are being sold. 

[ADD HERE]

```{r calc_price_usd}
cryptodata <- mutate(cryptodata, 
                     trade_usd_1 = ask_1_price * ask_1_quantity,
                     trade_usd_2 = ask_2_price * ask_2_quantity,
                     trade_usd_3 = ask_3_price * ask_3_quantity,
                     trade_usd_4 = ask_4_price * ask_4_quantity,
                     trade_usd_5 = ask_5_price * ask_5_quantity)
```

We can look at an example [ADD HERE]
```{r}
head(select(cryptodata, symbol, date_time_utc, ask_1_price, ask_1_quantity, trade_usd_1))
```


If none of the top 5 orders on the order book ask side are for at least $15, exclude the row. [ADD HERE]

```{r calculate_price_usd}
cryptodata <- mutate(cryptodata, 
                     price_usd = case_when(
                       cryptodata$trade_usd_1 >= 15 ~ cryptodata$ask_1_price,
                       cryptodata$trade_usd_2 >= 15 ~ cryptodata$ask_2_price,
                       cryptodata$trade_usd_3 >= 15 ~ cryptodata$ask_3_price,
                       cryptodata$trade_usd_4 >= 15 ~ cryptodata$ask_4_price,
                       cryptodata$trade_usd_5 >= 15 ~ cryptodata$ask_5_price))
```


Now remove rows where we couldn't find a price above $15 for any of the 5 cheapest orders in the order book.
```{r, include=F}
rowcount1 <- nrow(cryptodata)
```

```{r remove_null_price_usd}
cryptodata <- na.omit(cryptodata)
```

```{r, include=F}
rowcount2 <- nrow(cryptodata)
```

This step removed `r rowcount1-rowcount2` rows on the latest run.


## Any gaps?

[ADD HERE]

### Convert to tsibble

... the `tsibble` package [@R-tsibble]...

#### Convert to hourly data and get rid of minutes and seconds

... `anytime()` from the `anytime` package [@R-anytime]...

```{r}
cryptodata$ts_index <- anytime(paste0(cryptodata$pkDummy,':00:00')) 
```

... `distinct()` from the `dplyr` package [@R-dplyr]...
```{r}
cryptodata <- distinct(cryptodata, symbol, ts_index, .keep_all=TRUE)
```

... `as_tsibble()` from the `tsibble` package [@R-tsibble]...

```{r}
cryptodata <- as_tsibble(cryptodata, index=ts_index, key=symbol)
```

### Scan gaps

```{r}
scan_gaps(cryptodata)
```

### Fill gaps

```{r rowcount1, include=F}
rowcount1 <- nrow(cryptodata)
```

```{r fill_gaps}
cryptodata <- fill_gaps(cryptodata)
```

```{r rowcount2, include=F}
rowcount2 <- nrow(cryptodata)
```

Now looking at the data again, there are `r rowcount2-rowcount1` additional rows that were added as implicitly missing in the data:

```{r show_filled_cryptodata}
cryptodata
```

### Group Data

[ADD HERE]

```{r back_to_tibble}
cryptodata <- as_tibble(cryptodata)
```


[ADD HERE]

...`group_by()` from the `dplyr` [@R-dplyr] package...

```{r}
cryptodata <- group_by(cryptodata, symbol)
```

### Calculate Target

[ADD HERE]

Also adding lagged variables here! [ADD HERE]

...`mutate()` from the `dplyr` [@R-dplyr] package...


```{r}
cryptodata <- mutate(cryptodata, 
                     target_price_24h = lead(price_usd, 24, order_by=ts_index),
                     # Now all the lagged variables:
                     lagged_price_1h  = lag(price_usd, 1, order_by=ts_index),
                     lagged_price_2h  = lag(price_usd, 2, order_by=ts_index),
                     lagged_price_3h  = lag(price_usd, 3, order_by=ts_index),
                     lagged_price_6h  = lag(price_usd, 6, order_by=ts_index),
                     lagged_price_12h = lag(price_usd, 12, order_by=ts_index),
                     lagged_price_24h = lag(price_usd, 24, order_by=ts_index))
                     #lagged_price_3d  = lag(price_usd, 24*3, order_by=ts_index),
                     #lagged_price_7d  = lag(price_usd, 24*7, order_by=ts_index),
                     #lagged_price_14d = lag(price_usd, 24*14, order_by=ts_index),
                     #lagged_price_31d = lag(price_usd, 24*31, order_by=ts_index))
```

Here is an example showing the results for the subset of data related to the Ethereum cryptocurrency (symbol == 'ETH') showing 30 rows and the relevant columns we just calculated:
```{r}
print(select(filter(cryptodata, symbol == 'ETH'),date_time_utc, price_usd, lagged_price_1h, lagged_price_24h, target_price_24h), n=30)
```

The field `target_price_24h` should have the value of `price_usd` 24 hours into the future relative to the row of data. All the `lagged_` fields show the price from the past relative to `date_time_utc`.

## Remove subsets without enough rows

[ADD HERE]

```{r rm_subset_not_enough_rows}
cryptodata <- filter(cryptodata, n() >= 500)
```



## Cross Validation

[ADD HERE]

(explain step below)


NEW CV METHOD - need to explain:
```{r ts_cross_validation}
# Remove rows with null date_time_utc to exclude missing data from next steps
cryptodata <- drop_na(cryptodata, date_time_utc)
# Counts by symbol
cryptodata <- cryptodata %>% group_by(symbol) %>% mutate(tot_rows = n())
# Add row index by symbol
cryptodata <- mutate(arrange(cryptodata, date_time_utc), row_id = seq_along(date_time_utc))
# Calculate what rows belong in the first split
cryptodata <- cryptodata %>% mutate(split_rows_1 = as.integer(n()/5),
                                    split_rows_2 = as.integer(split_rows_1*2),
                                    split_rows_3 = as.integer(split_rows_1*3),
                                    split_rows_4 = as.integer(split_rows_1*4),
                                    split_rows_5 = as.integer(split_rows_1*5))
# Now calculate what split the current row_id belongs into
cryptodata <- mutate(cryptodata, 
                     split = case_when(
                       row_id <= split_rows_1 ~ 1,
                       row_id <= split_rows_2 ~ 2,
                       row_id <= split_rows_3 ~ 3,
                       row_id <= split_rows_4 ~ 4,
                       row_id > split_rows_4 ~ 5))
# Now figure out train/test groups
cryptodata <- cryptodata %>% mutate(train_rows_1 = (as.integer(n()/5))*0.8,
                                    test_rows_1  = train_rows_1 + (as.integer(n()/5))*0.2,
                                    train_rows_2 = test_rows_1 + train_rows_1,
                                    test_rows_2  = train_rows_2 + (as.integer(n()/5))*0.2,
                                    train_rows_3 = test_rows_2 + train_rows_1,
                                    test_rows_3  = train_rows_3 + (as.integer(n()/5))*0.2,
                                    train_rows_4 = test_rows_3 + train_rows_1,
                                    test_rows_4  = train_rows_4 + (as.integer(n()/5))*0.2,
                                    train_rows_5 = test_rows_4 + train_rows_1,
                                    test_rows_5  = train_rows_5 + (as.integer(n()/5))*0.2)
# Now assign train/test groups
cryptodata <- mutate(cryptodata, 
                     training = case_when(
                       row_id <= train_rows_1 ~ 'train',
                       row_id <= test_rows_1 ~ 'test',
                       row_id <= train_rows_2 ~ 'train',
                       row_id <= test_rows_2 ~ 'test',
                       row_id <= train_rows_3 ~ 'train',
                       row_id <= test_rows_3 ~ 'test',
                       row_id <= train_rows_4 ~ 'train',
                       row_id <= test_rows_4 ~ 'test',
                       row_id <= train_rows_5 ~ 'train',
                       row_id > train_rows_5 ~ 'holdout'))
# Remove all columns that are no longer needed now
cryptodata <- select(cryptodata, -(tot_rows:test_rows_5), -(trade_usd_1:trade_usd_5),
                     -(ask_1_price:bid_5_quantity), -pair, -quote_currency, -pkDummy, -pkey, split)
```



Our data now has the new columns `training` (*train*, *test* or *holdout*) and `split` (numbers 1-5) added to it, let's take a look at the new columns:

```{r cross_validate_preview_1}
select(cryptodata, training, split, date)
```
*Notice that even though we left `symbol` variables out of our selection, because it is part of the way we grouped our data, it was added back in with the message "Adding missing grouping variables `symbol`". The data is tied to its groupings when performing all operations until we use `ungroup()` to undo them.*

Let's add the new `split` column to the way the data is grouped:
```{r}
cryptodata <- group_by(cryptodata, symbol, split)
```


The new field `split`, helps us split the data into 5 different datasets based on the date, and contains a number from 1-5. The new field `training` flags the data as being part of the ***train*** dataset, or the ***test*** (or ***holdout*** for the first split) dataset for each of the 5 splits/datasets.

Running the same code as before with `tail()` added, we should see rows associated with the test data of the 5th split (again remember, each of the 5 splits has a training and testing dataset):
```{r cross_validate_preview_tail}
tail( select(cryptodata, training, split, date) )
```

The easiest way to understand these groupings, is to visualize them. In the next section, you will learn powerful tools for visualizing data in R. Do not worry if you do not understand the code below and are not familiar with `ggplot()`, we will explain this framework in the [next section](#visualization), for now review the charts below and try to follow along with the way we are grouping the data for the predictive models by looking at what the x and y axis represent, as well as the colors. On the x-axis we are plotting the DateTime of when a data point was collected, and on the y-axis the `split` (1-5) as described in this section. The data is then colored based on the category assigned for the `training` variable ("train","test" or "holdout").

We can visualize the new grouping variables:
```{r cv_groupings_visualized}
groups_chart <- ggplot(cryptodata,
                       aes(x = date_time_utc, y = split, color = training)) +
                       geom_point() #+
                       #scale_y_reverse()
# now show the chart we just saved:
groups_chart
```


We can check on the groupings for each cryptocurrency by animating the previous chart:
```{r animate_groupings, message=FALSE, warning=FALSE}
library(gganimate)
animated_chart <- groups_chart +
    transition_states(symbol) +
    ggtitle('Now showing: {closest_state}')
# show the new animated chart
animate(animated_chart, fps = 2)
```

If/when need to slow these down, use this code: (can't change fps from 1 - or at least be careful, it's error prone)
animate(animated_chart, fps = 1)

***This is another tool that we will walk through in the [next section](#visualization).***

It can be a bit hard to tell how many data points there are because they end up looking like lines. Let's change the plot to use `geom_jitter()` instead of `geom_point()`, which will manually offset the points and let us see exactly how many data points there are:
```{r gganimate_jitter, message=FALSE, warning=FALSE}
animated_chart <- animated_chart +
                    geom_jitter()
# show the new animated chart
animate(animated_chart, fps = 2)
```


## Nest data


[ADD HERE]

... explain goal and method

... First make sure groupings are correct

```{r group_for_nest}
cryptodata <- group_by(cryptodata, symbol, split, training)
```


Example nesting data:
```{r make_nested_ex}
nest(cryptodata) 
```


First make training data nested:
```{r nest_train}
cryptodata_train <- rename(nest(filter(cryptodata, training=='train')), train_data = 'data')
# Now remove training column
cryptodata_train <- select(ungroup(cryptodata_train, training), -training)
# Fix issues with individual groups of the data
cryptodata_train$train_data <- lapply(cryptodata_train$train_data, na.omit)
# Remove elements with no rows after na.omit step. CONFIRM THIS WORKS!!!
# First add new column with nrow of train dataset
cryptodata_train <- group_by(ungroup(mutate(rowwise(cryptodata_train), train_rows = nrow(train_data))), symbol, split)
# Remove all symbols where their train data has less than 20 rows at least once
symbols_rm <- unique(filter(cryptodata_train, train_rows < 20)$symbol)
# Remove all data relating to the symbols found above
cryptodata_train <- filter(cryptodata_train, ! symbol %in% symbols_rm) # ! is to make %not in% operator
# Drop train_rows column
cryptodata_train <- select(cryptodata_train, -train_rows)
```


Now nest test data:
```{r nest_test}
cryptodata_test <- select(rename(nest(filter(cryptodata, training=='test')), test_data = 'data'), -training)
# Now remove training column
cryptodata_test <- select(ungroup(cryptodata_test, training), -training)
```


Also do holdout:
```{r nest_holdout}
cryptodata_holdout <- rename(nest(filter(cryptodata, training=='holdout')), holdout_data = 'data')
# Remove split and training columns from holdout
cryptodata_holdout <- select(ungroup(cryptodata_holdout, split, training), -split, -training)
```


Now join all nested data into the same dataframe
```{r}
# Join train and test
cryptodata_nested <- left_join(cryptodata_train, cryptodata_test,by = c("symbol", "split"))
# Join holdout
cryptodata_nested <- left_join(cryptodata_nested, cryptodata_holdout, by = c("symbol"))
```


New data:
```{r show_nested}
cryptodata_nested
```



[ADD HERE - Intro to Visualization and explain we will use grouped data in PredictiveModeling]



[ADD HERE - Worth mentioning the fact that some data will be higher quality, etc...? Give more background on those steps as "catch-alls"?]



## Functional Programming

<!-- https://youtu.be/rz3_FDVt9eg?t=1407  -->

Could work with the data using for loops, which is an "object-oriented" approach. Basically, we could take our given option, iterate through every row of the data, and perform operations on each row and subset of the data we are interested in. That is one approach, but instead we will use a different approach using a "functional programming" approach



## Example Simple Model

...First need to run example normal lm model, etc...


### Using Functional Programming


```{r}
linear_model <- function(df){
  lm(target_price_24h ~ ., data = df) #NEED TO REMOVE 0 VARIANCE COLUMNS (WHICH ONES? ANY LEFT?)
}
```


Can now use it for `map()`
```{r}
cryptodata_nested %>% mutate(lm_model = map(train_data, linear_model))
```




## Caret

### Parallel Processing

[ADD HERE About R only using one CPU as deafault but can use more enabling parallel processing]

```{r parallel_processing}
library(doParallel)
cl <- makePSOCKcluster(12)
registerDoParallel(cl)
```


[ADD HERE]

Here use Caret + purrr to make models

```{r}
linear_model_caret <- function(df){

  train(price_usd ~ ., data = df,
        method = 'lm',
        trControl=trainControl(method="none"))

}
```

Can now use it for `map()`
```{r}
library(tictoc)
tic('linear model caret')
cryptodata_nested %>% mutate(lm_model = map(train_data, linear_model_caret))
toc()
```









Done with the parallel processing now:

```{r stop_parallel_processing}
stopCluster(cl)
```
